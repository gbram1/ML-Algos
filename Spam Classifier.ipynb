{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch data\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using email module to parse emails\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_email(is_spam, filename, spam_path = SPAM_PATH):\n",
    "    direc = \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, direc, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy = email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path: <exmh-workers-admin@spamassassin.taint.org>\n",
      "Delivered-To: zzzz@localhost.netnoteinc.com\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id D03E543C36\n",
      "\tfor <zzzz@localhost>; Thu, 22 Aug 2002 07:36:16 -0400 (EDT)\n",
      "Received: from phobos [127.0.0.1]\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 12:36:16 +0100 (IST)\n",
      "Received: from listman.spamassassin.taint.org (listman.spamassassin.taint.org\n",
      " [66.187.233.211]) by    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id\n",
      " g7MBYrZ04811 for    <zzzz-exmh@spamassassin.taint.org>; Thu, 22 Aug 2002\n",
      " 12:34:53 +0100\n",
      "Received: from listman.spamassassin.taint.org (localhost.localdomain\n",
      " [127.0.0.1]) by    listman.redhat.com (Postfix) with ESMTP id 8386540858;\n",
      " Thu, 22 Aug 2002    07:35:02 -0400 (EDT)\n",
      "Delivered-To: exmh-workers@listman.spamassassin.taint.org\n",
      "Received: from int-mx1.corp.spamassassin.taint.org\n",
      " (int-mx1.corp.spamassassin.taint.org    [172.16.52.254]) by\n",
      " listman.redhat.com (Postfix) with ESMTP id 10CF8406D7    for\n",
      " <exmh-workers@listman.redhat.com>; Thu, 22 Aug 2002 07:34:10 -0400    (EDT)\n",
      "Received: (from mail@localhost) by int-mx1.corp.spamassassin.taint.org\n",
      " (8.11.6/8.11.6)    id g7MBY7g11259 for exmh-workers@listman.redhat.com; Thu,\n",
      " 22 Aug 2002    07:34:07 -0400\n",
      "Received: from mx1.spamassassin.taint.org (mx1.spamassassin.taint.org\n",
      " [172.16.48.31]) by    int-mx1.corp.redhat.com (8.11.6/8.11.6) with SMTP id\n",
      " g7MBY7Y11255 for    <exmh-workers@redhat.com>; Thu, 22 Aug 2002 07:34:07\n",
      " -0400\n",
      "Received: from ratree.psu.ac.th ([202.28.97.6]) by mx1.spamassassin.taint.org\n",
      "    (8.11.6/8.11.6) with SMTP id g7MBIhl25223 for <exmh-workers@redhat.com>;\n",
      "    Thu, 22 Aug 2002 07:18:55 -0400\n",
      "Received: from delta.cs.mu.OZ.AU (delta.coe.psu.ac.th [172.30.0.98]) by\n",
      "    ratree.psu.ac.th (8.11.6/8.11.6) with ESMTP id g7MBWel29762;\n",
      "    Thu, 22 Aug 2002 18:32:40 +0700 (ICT)\n",
      "Received: from munnari.OZ.AU (localhost [127.0.0.1]) by delta.cs.mu.OZ.AU\n",
      "    (8.11.6/8.11.6) with ESMTP id g7MBQPW13260; Thu, 22 Aug 2002 18:26:25\n",
      "    +0700 (ICT)\n",
      "From: Robert Elz <kre@munnari.OZ.AU>\n",
      "To: Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "Cc: exmh-workers@spamassassin.taint.org\n",
      "Subject: Re: New Sequences Window\n",
      "In-Reply-To: <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "References: <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "    <1029882468.3116.TMDA@deepeddy.vircio.com> <9627.1029933001@munnari.OZ.AU>\n",
      "    <1029943066.26919.TMDA@deepeddy.vircio.com>\n",
      "    <1029944441.398.TMDA@deepeddy.vircio.com>\n",
      "MIME-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Message-Id: <13258.1030015585@munnari.OZ.AU>\n",
      "X-Loop: exmh-workers@spamassassin.taint.org\n",
      "Sender: exmh-workers-admin@spamassassin.taint.org\n",
      "Errors-To: exmh-workers-admin@spamassassin.taint.org\n",
      "X-Beenthere: exmh-workers@spamassassin.taint.org\n",
      "X-Mailman-Version: 2.0.1\n",
      "Precedence: bulk\n",
      "List-Help: <mailto:exmh-workers-request@spamassassin.taint.org?subject=help>\n",
      "List-Post: <mailto:exmh-workers@spamassassin.taint.org>\n",
      "List-Subscribe:\n",
      " <https://listman.spamassassin.taint.org/mailman/listinfo/exmh-workers>,   \n",
      " <mailto:exmh-workers-request@redhat.com?subject=subscribe>\n",
      "List-Id:\n",
      " Discussion list for EXMH developers <exmh-workers.spamassassin.taint.org>\n",
      "List-Unsubscribe:\n",
      " <https://listman.spamassassin.taint.org/mailman/listinfo/exmh-workers>,   \n",
      " <mailto:exmh-workers-request@redhat.com?subject=unsubscribe>\n",
      "List-Archive:\n",
      " <https://listman.spamassassin.taint.org/mailman/private/exmh-workers/>\n",
      "Date: Thu, 22 Aug 2002 18:26:25 +0700\n",
      "\n",
      "    Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
      "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "\n",
      "  | I can't reproduce this error.\n",
      "\n",
      "For me it is very repeatable... (like every time, without fail).\n",
      "\n",
      "This is the debug log of the pick happening ...\n",
      "\n",
      "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
      "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
      "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
      "18:19:04 Marking 1 hits\n",
      "18:19:04 tkerror: syntax error in expression \"int ...\n",
      "\n",
      "Note, if I run the pick command by hand ...\n",
      "\n",
      "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
      "1 hit\n",
      "\n",
      "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
      "using is ...\n",
      "\n",
      "delta$ pick -version\n",
      "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
      "\n",
      "And the relevant part of my .mh_profile ...\n",
      "\n",
      "delta$ mhparam pick\n",
      "-seq sel -list\n",
      "\n",
      "\n",
      "Since the pick command works, the sequence (actually, both of them, the\n",
      "one that's explicit on the command line, from the search popup, and the\n",
      "one that comes from .mh_profile) do get created.\n",
      "\n",
      "kre\n",
      "\n",
      "ps: this is still using the version of the code form a day ago, I haven't\n",
      "been able to reach the cvs repository today (local routing issue I think).\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "Exmh-workers mailing list\n",
      "Exmh-workers@redhat.com\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ham_emails = [load_email(is_spam = False, filename = name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam = True, filename = name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin A posted:\n",
      "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
      " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
      " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
      " \n",
      " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
      " museum, a restored amphitheatre and car park for admiring crowds are\n",
      "planned\n",
      "---------------------\n",
      "So is this mountain limestone or granite?\n",
      "If it's limestone, it'll weather pretty fast.\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    #return email if it is \n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join(\n",
    "            [get_email_structure(sub_email) for sub_email in payload]))\n",
    "    else: return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counter of each type of email\n",
    "from collections import Counter\n",
    "def structures_counter(emails):\n",
    "    structures = Counter() #create a counter dictionary\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email) #get the stucture of the email\n",
    "        structures[structure] += 1 #increment counter\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <12a1mailbot1@web.de>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32\tfor <zzzz@localhost>; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\n",
      "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\n",
      "Received : from dd_it7 ([210.97.77.167])\tby webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 13:09:41 +0100\n",
      "From : 12a1mailbot1@web.de\n",
      "Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);\t Sat, 24 Aug 2002 09:42:10 +0900\n",
      "To : dcek1a1@netsgo.com\n",
      "Subject : Life Insurance - Why Pay More?\n",
      "Date : Wed, 21 Aug 2002 20:31:57 -1600\n",
      "MIME-Version : 1.0\n",
      "Message-ID : <0103c1042001882DD_IT7@dd_it7>\n",
      "Content-Type : text/html; charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding : quoted-printable\n"
     ]
    }
   ],
   "source": [
    "#Email headers\n",
    "for header, value in spam_emails[0].items():\n",
    "    print(header,\":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zzzz@localhost', 'spamassassin', 'taint', 'org']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[0][\"Delivered-To\"].split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split out training and test sets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1]*len(spam_emails))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ÿFFFFA9 \n",
      "      Copyright 2002 - All rights reservedIf you would no longer like us \n",
      "      to contact you or feel that you havereceived this email in error, \n",
      "      please click here to \n",
      "      unsubscribe.\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Preprocess the data\n",
    "#Get rid of HTML using BeautifulSoup instead of custom transformer\n",
    "from bs4 import BeautifulSoup\n",
    "html_spam_emails = [email for email in X_train[y_train == 1]\n",
    "                   if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_email = html_spam_emails[7]\n",
    "#print(sample_html_email.get_content())\n",
    "\n",
    "soup = BeautifulSoup(sample_html_email.get_content(), 'lxml')\n",
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: #if there's encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == 'test/plain':\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        new_html = BeautifulSoup(html, 'lxml')\n",
    "        return new_html.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ÿFFFFA9 \n",
      "      Copyright 2002 - All rights reservedIf you would no longer like us \n",
      "      to contact you or feel that you havereceived this email in error, \n",
      "      please click here to \n",
      "      unsubscribe.\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://google.com', 'yo.io']\n"
     ]
    }
   ],
   "source": [
    "#Replace URLs with the word 'URL'\n",
    "import urlextract\n",
    "url_extractor = urlextract.URLExtract()\n",
    "print(url_extractor.find_urls(\"check check https://google.com and yo.io\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom transformer to convert emails into word counters\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers = True, lower_case = True, remove_punctuation = True,\n",
    "                replace_urls = True, replace_numbers = True, stemming = True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    \n",
    "    def fit(self, X, y = None): return self\n",
    "    def transform(self, X, y = None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key = lambda url: len(url), reverse = True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "            \n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'url': 2, 'for': 2, 'url:': 1, 'date:': 1, 'number-number-numbertnumber:number:number-numb': 1, '...': 1, 'not': 1, 'updating:': 1, \"i'm\": 1, 'do': 1, 'the': 1, 'guestblog': 1, 'at': 1, 'bo': 1, 'boing[number].': 1, 'now': 1, 'to': 1, 'find': 1, 'an': 1, 'excus': 1, 'miss': 1, 'last': 1, 'week.': 1, '[number]': 1}),\n",
       "       Counter({'on': 4, 'to': 4, 'use': 3, 'thi': 3, 'septemb': 3, 'url': 3, 'you': 3, 'messag': 3, 'your': 3, 'perl': 2, 'post': 2, 'by': 2, 'monday': 2, 'number,': 2, '@number:numb': 2, 'receiv': 2, 'and': 2, 'from': 2, 'or': 2, 'chang': 2, 'daili': 1, 'headlin': 1, 'mailer': 1, 'week': 1, 'perlnumber-port': 1, '(number': 1, 'august': 1, '/': 1, 'numberst': 1, 'number)': 1, 'rafael': 1, '(summaries)': 1, 'the': 1, 'review,': 1, 'vnumber': 1, 'inumb': 1, 'ziggi': 1, '(news)': 1, 'copyright': 1, 'number-numb': 1, 'pudge.': 1, 'all': 1, 'right': 1, 'reserved.': 1, '======================================================================': 1, 'have': 1, 'becaus': 1, 'subscrib': 1, 'it': 1, 'perl.': 1, 'stop': 1, 'other': 1, 'perl,': 1, 'add': 1, 'more': 1, 'preferences,': 1, 'pleas': 1, 'go': 1, 'user': 1, 'page.': 1, 'can': 1, 'log': 1, 'in': 1, 'prefer': 1, 'there.': 1}),\n",
       "       Counter({'>': 16, 'a': 3, '\"i': 2, 'not': 2, 'have': 2, 'that': 2, 'from:': 2, 'fork': 2, '-----': 2, 'onc': 2, 'said': 2, 'did': 1, 'sex': 1, 'with': 1, 'woman.\"': 1, '-----origin': 1, 'message-----': 1, 'fork-admin@xent.com': 1, '[mailto:fork-admin@xent.com]': 1, 'on': 1, 'behalf': 1, 'of': 1, 'mr.': 1, 'sent:': 1, 'monday,': 1, 'septemb': 1, 'number,': 1, 'number': 1, 'number:numb': 1, 'pm': 1, 'to:': 1, 'subject:': 1, 're:': 1, 'goodby': 1, 'global': 1, 'warm': 1, 'origin': 1, 'messag': 1, '\"john': 1, 'hall\"': 1, 'green': 1, 'if': 1, 'the': 1, 'spot': 1, 'owl': 1, \"hadn't\": 1, 'exist': 1, 'they': 1, 'would': 1, 'had': 1, 'to': 1, 'invent': 1, 'it.': 1, 'republican': 1, 'am': 1, 'crook\".': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => comput\n",
      "Computation => comput\n",
      "Computing => comput\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))\n",
    "except ImportError:\n",
    "    print(\"Error: stemming requires the NLTK module.\")\n",
    "    stemmer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the word count into a sparse matrix \n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class VectorizeWordCount(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size = 10000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    \n",
    "    #Fit method will build the vocab list by most frequent\n",
    "    def fit(self, X, y = None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "    #Transform method will convert counts to vectors\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21,  0,  1,  2,  0,  0,  0,  1,  1,  0,  0],\n",
       "       [72,  0,  4,  3,  4,  3,  3,  0,  1,  3,  3],\n",
       "       [59, 16,  1,  0,  1,  1,  1,  2,  1,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformed = VectorizeWordCount(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformed.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'>': 1,\n",
       " 'to': 2,\n",
       " 'url': 3,\n",
       " 'on': 4,\n",
       " 'septemb': 5,\n",
       " 'messag': 6,\n",
       " 'not': 7,\n",
       " 'the': 8,\n",
       " 'use': 9,\n",
       " 'thi': 10}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformed.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", VectorizeWordCount()),\n",
    "])\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielbram/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
      "/Users/gabrielbram/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.981, total=   0.3s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.988, total=   0.2s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.988, total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielbram/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9854166666666666"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train on logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver = \"lbfgs\", random_state = 0)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv = 3, verbose = 3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielbram/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:389: UserWarning: \"http://www.post-gazette.com/columnists/20020905brian5.asp\n",
      "\n",
      "\n",
      "\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/gabrielbram/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-a3f52e9fdf4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#fit the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Precision and Recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "#check precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "log_clf = LogisticRegression(solver = \"lbfgs\", random_state = 0)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "#fit the test set\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "print(y_pred.score())\n",
    "\n",
    "#Precision and Recall\n",
    "print(\"Precision: \", precision_score(y_test, y_pred))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
